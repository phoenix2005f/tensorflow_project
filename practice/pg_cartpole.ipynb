{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phx/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Gradient:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        n_states, \n",
    "        gamma = 0.9, #遠見程度\n",
    "        epsilon = None,  #保守程度，越大就越容易用Q值大小來採取行動；越小則越容易產生隨機行動\n",
    "        epsilon_increase = None,\n",
    "        learning_rate = 0.001, #神經網路的更新率\n",
    "        #memory_size = 50, #####\n",
    "        #batch_size = 32, #####\n",
    "        nueron_num = 10\n",
    "    ):\n",
    "    \n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.gamma = gamma\n",
    "        #self.epsilon_max = epsilon #####\n",
    "        #self.epsilon_increase = epsilon_increase #####\n",
    "        #self.epsilon = 0 if epsilon_increase is not None else epsilon #####\n",
    "        self.lr = learning_rate\n",
    "        #self.memory_size = memory_size #####\n",
    "        #self.memory_counter = 0 #####\n",
    "        #self.batch_size = batch_size ####\n",
    "        self.nueron_num = nueron_num\n",
    "        \n",
    "        ##### initialize memory\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.past_state, self.past_action, self.past_reward = [], [], []\n",
    "        self.action_one_hot = np.zeros(self.n_actions, dtype=np.int32)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        tf.reset_default_graph() ## 重新 build graph 需要跑這行\n",
    "        self.sess = tf.Session() #宣告session\n",
    "        #輸入current state\n",
    "        self.state_input = tf.placeholder(shape = [None, self.n_states], \n",
    "                                          name = 'state_input',\n",
    "                                          dtype = tf.float32)\n",
    "        \"\"\"\n",
    "        輸入real action和神經網路的output act_proba算cross entropy當作更新方向\n",
    "        以超級瑪莉的遊戲為例 action = [上, 下, 左, 右] 如果實際action為向左則\n",
    "        action = [0, 0, 1, 0]。\n",
    "        也可以將這四個動作用0, 1, 2 ,3代表，如此的話只需要用一維來存取動作，也就是輸\n",
    "        入shape = [None, 1]，那後面再算cross entropy的話就要用tf.nn.sparse_\n",
    "        softmax_cross_entropy_with_logits，大家也可以試著改寫看看。\n",
    "        \"\"\"    \n",
    "        self.real_action = tf.placeholder(shape = [None, self.n_actions], \n",
    "                                          name = 'real_action',\n",
    "                                          dtype = tf.float32)\n",
    "        \"\"\"\n",
    "        但是有時候產生的動作會帶來好的效果或壞的效果並且程度不一，因此loss不能光用神經網路的\n",
    "        輸出action_proba和real action的cross entropy代表，因此這邊乘上action_reward\n",
    "        來校正loss。例如某個動作很有幫助那必然會產生很大的action_reward，因此乘上很大的\n",
    "        action_reward即可加大loss讓此動作之後產生的機率被放大；相反的，某個動作如果產生很\n",
    "        好的效果反而會帶來負的action_reward使得loss變負的，讓更新方向相反使得之後輸出此動\n",
    "        作的機會減少。\n",
    "        \"\"\"\n",
    "        self.Vt = tf.placeholder(shape= [None, ], \n",
    "                                            name=\"Vt\",\n",
    "                                            dtype = tf.float32)\n",
    "        #搭建神經網路\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.act_proba = self.build_network(self.nueron_num, Trainable = True, \\\n",
    "                             scope = 'net_eval') \n",
    "            \n",
    "        \n",
    "        #管理神經網路的parameters\n",
    "        self.Actor_eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/net_eval')\n",
    "        \n",
    "        \n",
    "        #loss\n",
    "        \"\"\"\n",
    "        算出 “神經網路輸出的動作機率”與 “實際動作”的cross entropy當作loss，但是更新的方向和力道就利用action\n",
    "        reward來決定。例如，這一回合產生的所有動作組合如果得到很好的reward，那就應該讓神經網路的輸出機率更靠近實\n",
    "        際輸出的結果，因此cross_entropy和action_reward相乘的到的loss就更大，更新力度就更大。相反的，這一回\n",
    "        合產生的所有動作組合如果得到負的reward，那就應該讓神經網路輸出動作的機率更遠離實際輸出結果，在這樣的狀況\n",
    "        下，cross_entropy和action_reward相乘的到的loss就會得到負的，神經網路的參數更新方向就會往反方向。\n",
    "        \"\"\"\n",
    "        self.cross_entropy = tf.reduce_sum(-tf.log(self.act_proba)*self.real_action, axis=1)\n",
    "        self.loss = tf.reduce_sum(self.cross_entropy*self.Vt)\n",
    " \n",
    "        \n",
    "        self.train = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, var_list=self.Actor_eval_params)\n",
    "    \n",
    "        self.sess.run(tf.global_variables_initializer()) #將神經網路初始化\n",
    "    \n",
    "    def write_memory(self, current_state, reward, action): #####\n",
    "        \n",
    "        ## past_action 要存one hot\n",
    "        action_one_hot = self.action_one_hot.copy()\n",
    "        action_one_hot[action]=1\n",
    "        \n",
    "        self.past_state.append(current_state)\n",
    "        self.past_action.append(action_one_hot)\n",
    "        self.past_reward.append(reward)\n",
    "    \n",
    "    \n",
    "    def build_network(self, neuron_num, Trainable, scope): \n",
    "         with tf.variable_scope(scope):\n",
    "            init_w , init_b = tf.random_normal_initializer(0.,0.2) , tf.constant_initializer(0.1)\n",
    "            \n",
    "            fc1 = tf.layers.dense(self.state_input , neuron_num , activation=tf.nn.tanh , \\\n",
    "                                  kernel_initializer = init_w , bias_initializer = init_b ,trainable = Trainable)\n",
    "            \n",
    "            fc2 = tf.layers.dense(fc1 , neuron_num , activation=tf.nn.tanh , \\\n",
    "                                  kernel_initializer = init_w , bias_initializer = init_b ,trainable = Trainable)\n",
    " \n",
    "            output = tf.layers.dense(inputs = fc2, units = self.n_actions, \\\n",
    "                activation = tf.nn.softmax, kernel_initializer=init_w, \\\n",
    "                bias_initializer=init_b, trainable=Trainable)\n",
    "        \n",
    "         return output\n",
    "    \n",
    "    \n",
    "    def choose_action(self , current_state):\n",
    "        act_prob = self.sess.run(self.act_proba, feed_dict={self.state_input : current_state[np.newaxis,:]})\n",
    "        \n",
    "   \n",
    "        act = np.random.choice(range(act_prob.shape[1]) , p = np.reshape(act_prob,-1) )\n",
    "        \n",
    "        return act\n",
    "        \n",
    "    \n",
    "    def learn(self): #####\n",
    "        vt = self.calculate_Vt(self.past_reward)\n",
    "\n",
    "        self.sess.run(self.train,feed_dict={\n",
    "             self.state_input: self.past_state,  # shape=[None, n_state]\n",
    "             self.real_action: self.past_action,  # shape=[None, n_actions]\n",
    "             self.Vt: vt  # shape=[None, ]\n",
    "        })\n",
    "        \n",
    "        #清空memory entry\n",
    "        self.past_state , self.past_action , self.past_reward = [],[],[]\n",
    "        \n",
    "        \n",
    "    \n",
    "    def calculate_Vt(self,reward):\n",
    "        \n",
    "        vt = np.zeros_like(reward)\n",
    "        reward_temp=0\n",
    "        for i in reversed(range(len(reward))):\n",
    "            reward_temp = reward[i] + reward_temp* self.gamma\n",
    "            vt[i] = reward_temp\n",
    "            \n",
    "        ## normalize reward\n",
    "        vt -= np.mean(vt)\n",
    "        vt /= np.std(vt)\n",
    "        \n",
    "        \n",
    "        return vt\n",
    "        \n",
    "    def model_save(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, \"saved_models/{}.ckpt\".format(model_name))\n",
    "    \n",
    "    def model_restore(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, \"saved_models/{}.ckpt\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(save_model, model_name):\n",
    "    step_record = []\n",
    "    #dead_record = []\n",
    "    for episode in range(200):\n",
    "        # initial environment並給出起始的state\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            # 產生環境視窗\n",
    "            env.render()\n",
    "\n",
    "            # 根據現在的狀態選擇動作\n",
    "            action = RL.choose_action(current_state)\n",
    "\n",
    "            # 產生動作和環境互動後產生下一個狀態、獎勵值及遊戲是否結束\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward+= reward\n",
    "            \n",
    "            \n",
    "            # 將資訊存至記憶體中以便進行experience replay\n",
    "            RL.write_memory(current_state, reward, action)\n",
    "            \n",
    "            #if reward < 0 :\n",
    "            #    dead+=1\n",
    "            # swap state\n",
    "            current_state = next_state\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                RL.learn()\n",
    "                print('episode:{} steps:{} total_reward:{}'.format(episode, step, total_reward))\n",
    "                step_record.append(step)\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "    # end of game\n",
    "    if save_model:\n",
    "        RL.model_save(model_name)\n",
    "    print('game over')\n",
    "    env.close()\n",
    "    return step_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 steps:18 total_reward:19.0\n",
      "episode:1 steps:41 total_reward:42.0\n",
      "episode:2 steps:15 total_reward:16.0\n",
      "episode:3 steps:28 total_reward:29.0\n",
      "episode:4 steps:36 total_reward:37.0\n",
      "episode:5 steps:26 total_reward:27.0\n",
      "episode:6 steps:56 total_reward:57.0\n",
      "episode:7 steps:22 total_reward:23.0\n",
      "episode:8 steps:15 total_reward:16.0\n",
      "episode:9 steps:9 total_reward:10.0\n",
      "episode:10 steps:12 total_reward:13.0\n",
      "episode:11 steps:13 total_reward:14.0\n",
      "episode:12 steps:26 total_reward:27.0\n",
      "episode:13 steps:12 total_reward:13.0\n",
      "episode:14 steps:19 total_reward:20.0\n",
      "episode:15 steps:40 total_reward:41.0\n",
      "episode:16 steps:49 total_reward:50.0\n",
      "episode:17 steps:23 total_reward:24.0\n",
      "episode:18 steps:10 total_reward:11.0\n",
      "episode:19 steps:19 total_reward:20.0\n",
      "episode:20 steps:19 total_reward:20.0\n",
      "episode:21 steps:17 total_reward:18.0\n",
      "episode:22 steps:38 total_reward:39.0\n",
      "episode:23 steps:16 total_reward:17.0\n",
      "episode:24 steps:9 total_reward:10.0\n",
      "episode:25 steps:10 total_reward:11.0\n",
      "episode:26 steps:10 total_reward:11.0\n",
      "episode:27 steps:52 total_reward:53.0\n",
      "episode:28 steps:30 total_reward:31.0\n",
      "episode:29 steps:46 total_reward:47.0\n",
      "episode:30 steps:15 total_reward:16.0\n",
      "episode:31 steps:21 total_reward:22.0\n",
      "episode:32 steps:20 total_reward:21.0\n",
      "episode:33 steps:29 total_reward:30.0\n",
      "episode:34 steps:34 total_reward:35.0\n",
      "episode:35 steps:30 total_reward:31.0\n",
      "episode:36 steps:32 total_reward:33.0\n",
      "episode:37 steps:17 total_reward:18.0\n",
      "episode:38 steps:15 total_reward:16.0\n",
      "episode:39 steps:15 total_reward:16.0\n",
      "episode:40 steps:51 total_reward:52.0\n",
      "episode:41 steps:13 total_reward:14.0\n",
      "episode:42 steps:8 total_reward:9.0\n",
      "episode:43 steps:11 total_reward:12.0\n",
      "episode:44 steps:42 total_reward:43.0\n",
      "episode:45 steps:24 total_reward:25.0\n",
      "episode:46 steps:31 total_reward:32.0\n",
      "episode:47 steps:24 total_reward:25.0\n",
      "episode:48 steps:13 total_reward:14.0\n",
      "episode:49 steps:81 total_reward:82.0\n",
      "episode:50 steps:94 total_reward:95.0\n",
      "episode:51 steps:23 total_reward:24.0\n",
      "episode:52 steps:32 total_reward:33.0\n",
      "episode:53 steps:26 total_reward:27.0\n",
      "episode:54 steps:19 total_reward:20.0\n",
      "episode:55 steps:57 total_reward:58.0\n",
      "episode:56 steps:96 total_reward:97.0\n",
      "episode:57 steps:34 total_reward:35.0\n",
      "episode:58 steps:40 total_reward:41.0\n",
      "episode:59 steps:35 total_reward:36.0\n",
      "episode:60 steps:68 total_reward:69.0\n",
      "episode:61 steps:117 total_reward:118.0\n",
      "episode:62 steps:30 total_reward:31.0\n",
      "episode:63 steps:24 total_reward:25.0\n",
      "episode:64 steps:40 total_reward:41.0\n",
      "episode:65 steps:62 total_reward:63.0\n",
      "episode:66 steps:49 total_reward:50.0\n",
      "episode:67 steps:72 total_reward:73.0\n",
      "episode:68 steps:57 total_reward:58.0\n",
      "episode:69 steps:98 total_reward:99.0\n",
      "episode:70 steps:41 total_reward:42.0\n",
      "episode:71 steps:78 total_reward:79.0\n",
      "episode:72 steps:40 total_reward:41.0\n",
      "episode:73 steps:116 total_reward:117.0\n",
      "episode:74 steps:34 total_reward:35.0\n",
      "episode:75 steps:51 total_reward:52.0\n",
      "episode:76 steps:94 total_reward:95.0\n",
      "episode:77 steps:124 total_reward:125.0\n",
      "episode:78 steps:112 total_reward:113.0\n",
      "episode:79 steps:29 total_reward:30.0\n",
      "episode:80 steps:56 total_reward:57.0\n",
      "episode:81 steps:72 total_reward:73.0\n",
      "episode:82 steps:42 total_reward:43.0\n",
      "episode:83 steps:66 total_reward:67.0\n",
      "episode:84 steps:90 total_reward:91.0\n",
      "episode:85 steps:67 total_reward:68.0\n",
      "episode:86 steps:69 total_reward:70.0\n",
      "episode:87 steps:113 total_reward:114.0\n",
      "episode:88 steps:85 total_reward:86.0\n",
      "episode:89 steps:168 total_reward:169.0\n",
      "episode:90 steps:46 total_reward:47.0\n",
      "episode:91 steps:36 total_reward:37.0\n",
      "episode:92 steps:61 total_reward:62.0\n",
      "episode:93 steps:261 total_reward:262.0\n",
      "episode:94 steps:444 total_reward:445.0\n",
      "episode:95 steps:69 total_reward:70.0\n",
      "episode:96 steps:87 total_reward:88.0\n",
      "episode:97 steps:71 total_reward:72.0\n",
      "episode:98 steps:95 total_reward:96.0\n",
      "episode:99 steps:70 total_reward:71.0\n",
      "episode:100 steps:91 total_reward:92.0\n",
      "episode:101 steps:103 total_reward:104.0\n",
      "episode:102 steps:120 total_reward:121.0\n",
      "episode:103 steps:82 total_reward:83.0\n",
      "episode:104 steps:150 total_reward:151.0\n",
      "episode:105 steps:97 total_reward:98.0\n",
      "episode:106 steps:96 total_reward:97.0\n",
      "episode:107 steps:62 total_reward:63.0\n",
      "episode:108 steps:137 total_reward:138.0\n",
      "episode:109 steps:139 total_reward:140.0\n",
      "episode:110 steps:301 total_reward:302.0\n",
      "episode:111 steps:170 total_reward:171.0\n",
      "episode:112 steps:131 total_reward:132.0\n",
      "episode:113 steps:145 total_reward:146.0\n",
      "episode:114 steps:566 total_reward:567.0\n",
      "episode:115 steps:1884 total_reward:1885.0\n",
      "episode:116 steps:450 total_reward:451.0\n",
      "episode:117 steps:516 total_reward:517.0\n",
      "episode:118 steps:591 total_reward:592.0\n",
      "episode:119 steps:384 total_reward:385.0\n",
      "episode:120 steps:814 total_reward:815.0\n",
      "episode:121 steps:1029 total_reward:1030.0\n",
      "episode:122 steps:550 total_reward:551.0\n",
      "episode:123 steps:475 total_reward:476.0\n",
      "episode:124 steps:796 total_reward:797.0\n",
      "episode:125 steps:15390 total_reward:15391.0\n",
      "episode:126 steps:3518 total_reward:3519.0\n",
      "episode:127 steps:720 total_reward:721.0\n",
      "episode:128 steps:6243 total_reward:6244.0\n",
      "episode:129 steps:93823 total_reward:93824.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-feefecfdfe42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                    \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                  )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mstep_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PG_try'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mstep_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fd865b511608>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(save_model, model_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# 根據現在的狀態選擇動作\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# 產生動作和環境互動後產生下一個狀態、獎勵值及遊戲是否結束\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-28a11d0b35b2>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, current_state)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step_result = []\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)     # reproducible, general Policy gradient has high variance\n",
    "env = env.unwrapped\n",
    "RL = Policy_Gradient(n_actions = 2, \n",
    "                   n_states = 4,\n",
    "                   gamma = 0.99,\n",
    "                   learning_rate = 0.01,\n",
    "                 )\n",
    "step_record = training(save_model = True, model_name='PG_try')\n",
    "step_result.append(pd.DataFrame(data = step_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_result = pd.DataFrame(reward_record)\n",
    "reward_result.columns = ['Policy Gradient']\n",
    "reward_result.plot()\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
