{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym_pull'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dda22a61925a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym_pull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym_pull'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_pull]\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnregisteredEnv",
     "evalue": "No registered env with id: SuperMarioBros-1-1-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Users/phx/anaconda/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SuperMarioBros-1-1-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-747b6e32ea65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SuperMarioBros-1-1-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/phx/anaconda/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/phx/anaconda/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/phx/anaconda/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Env {} not found (valid versions include {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No registered env with id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m: No registered env with id: SuperMarioBros-1-1-v0"
     ]
    }
   ],
   "source": [
    "env = gym.make('SuperMarioBros-1-1-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPISILON = 0.9\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "MEMORY_CAPACITY = 150\n",
    "GAMMA = 0.9\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dueling_double_DQN_Agent():\n",
    "    def __init__(self,\n",
    "                 n_actions = 2,\n",
    "                 n_states = 4,\n",
    "                 memory_capacity = 150,\n",
    "                 episilon = 0.9,\n",
    "                 gamma = 0.9,\n",
    "                 batch_size = 32,\n",
    "                 lr = 0.01,\n",
    "                 memory_counter = 0,\n",
    "                 synchronize_iter = 100,\n",
    "                 \n",
    "                 ):\n",
    "        \n",
    "        self.memory = np.zeros((memory_capacity,n_states*2+2))\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.memory_counter = 0\n",
    "        self.synchronize_iter = synchronize_iter\n",
    "        self.synchronize_counter = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.episilon = episilon\n",
    "        \n",
    "        self._build_net()\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_net(self):\n",
    "        self.s = tf.placeholder(tf.float32,[None,self.n_states],name = \"current_state\")\n",
    "        self.a = tf.placeholder(tf.int32,[None,],name = \"action\")\n",
    "        self.r = tf.placeholder(tf.float32,[None,] ,name = \"reward\")\n",
    "        self.s_ = tf.placeholder(tf.float32,[None,self.n_states],name = \"next_state\")\n",
    "        \n",
    "        init_weight,init_bias = tf.truncated_normal_initializer(stddev=0.01),tf.constant_initializer(0.1)\n",
    "        \n",
    "        \n",
    "        ######## dueling 要改網路架構  ########\n",
    "        \n",
    "        # eval_net\n",
    "        with tf.variable_scope(\"eval_net\"):\n",
    "            \n",
    "            self.e_fc1 = tf.layers.dense(self.s , 50 , activation = tf.nn.relu , \n",
    "                            kernel_initializer = init_weight , \n",
    "                            bias_initializer = init_bias, name=\"e_fc1\")\n",
    "            \n",
    "            self.e_value = tf.layers.dense(self.e_fc1 , 1 , activation = tf.nn.relu , \n",
    "                            kernel_initializer = init_weight , \n",
    "                            bias_initializer = init_bias, name=\"e_value\")\n",
    "            \n",
    "            self.e_act_q = tf.layers.dense(self.e_fc1 , self.n_actions , activation=tf.nn.relu,\n",
    "                                   kernel_initializer = init_weight,\n",
    "                                   bias_initializer = init_bias , \n",
    "                                    name=\"e_fc2\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        # target_net\n",
    "        with tf.variable_scope(\"target_net\"):\n",
    "            self.t_fc1 = tf.layers.dense(self.s_ , 50 , activation =tf.nn.relu,\n",
    "                                        kernel_initializer = init_weight , \n",
    "                                        bias_initializer = init_bias , name=\"t_fc1\")\n",
    "            \n",
    "            self.t_value = tf.layers.dense(self.t_fc1 , 1 , activation = tf.nn.relu , \n",
    "                kernel_initializer = init_weight , \n",
    "                bias_initializer = init_bias, name=\"t_value\")\n",
    "            \n",
    "            self.t_act_q = tf.layers.dense(self.t_fc1 , self.n_actions , activation=tf.nn.relu,\n",
    "                                        kernel_initializer = init_weight,\n",
    "                                        bias_initializer = init_bias , name= \"t_fc2\")\n",
    "        \n",
    "        \n",
    "        ######################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.eval_params = tf.get_collection(key = tf.GraphKeys.TRAINABLE_VARIABLES , scope = \"eval_net\")\n",
    "        self.target_params = tf.get_collection(key = tf.GraphKeys.TRAINABLE_VARIABLES , scope = \"target_net\")\n",
    "        \n",
    "        \n",
    "        # eval 參數同步到 target的op\n",
    "        self.synchronize_op = [ tf.assign(t,e) for e,t in zip(self.eval_params,self.target_params)]\n",
    "        \n",
    "        # loss\n",
    "        e_m = tf.reduce_mean(self.e_act_q, axis=1, keep_dims=True)\n",
    "        e_devs_squared = tf.square(self.e_act_q - e_m)\n",
    "        e_std = tf.reduce_mean(e_devs_squared, axis=1, keep_dims=True)\n",
    "        \n",
    "        t_m = tf.reduce_mean(self.t_act_q, axis=1, keep_dims=True)\n",
    "        t_devs_squared = tf.square(self.t_act_q - t_m)\n",
    "        t_std = tf.reduce_mean(t_devs_squared, axis=1, keep_dims=True)\n",
    "        \n",
    "        \n",
    "        ### 理論上這邊第二項要standarize (減掉平均值 再除以 標準差),但反而會train壞,有待改進\n",
    "        self.e_out = self.e_value + self.e_act_q\n",
    "        self.t_out = self.t_value + self.t_act_q\n",
    "        \n",
    "        \n",
    "        ######### double QDN只要改這邊 #########\n",
    "        # 用eval net得到的batch action,再利用target net得到batch action所對應的Q值\n",
    "        # 會加速不少\n",
    "        \n",
    "        e_actions = tf.argmax(self.e_out,axis=1,output_type=tf.int32)\n",
    "        e_actions_idx = tf.stack([tf.range(tf.shape(e_actions)[0]),e_actions],axis=1)\n",
    "        next_q=tf.gather_nd(params=self.t_out,indices = e_actions_idx)\n",
    "        \n",
    "        ######################################\n",
    "        \n",
    "        self.target_q = self.r + self.gamma*next_q\n",
    "#         self.target_q = tf.stop_gradient(self.target_q)  # target不要更新參數\n",
    "        \n",
    "        index = tf.stack([tf.range(tf.shape(self.a)[0]),self.a],axis=1)\n",
    "        self.eval_q = tf.gather_nd(params = self.e_out , indices = index)\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.eval_q,self.target_q))\n",
    "        \n",
    "        # 只更新eval net的參數\n",
    "        q_vars = [ var for var in tf.trainable_variables() if \"eval_net\" in var.name]\n",
    "        self.step = tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.loss,var_list=q_vars)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def choose_action(self,states):\n",
    "        states = states[np.newaxis,:]\n",
    "        \n",
    "        q_val = self.sess.run(self.e_out,feed_dict={self.s:states})\n",
    "        if np.random.normal() < self.episilon:\n",
    "            act = np.argmax(q_val,axis=1)[0]\n",
    "        else:\n",
    "            act = np.random.choice(self.n_actions)\n",
    "        return act\n",
    "        \n",
    "    def store_entry(self,s,a,r,s_):\n",
    "        idx = self.memory_counter % self.memory_capacity\n",
    "        \n",
    "        row_entry = np.hstack([s,a,r,s_])\n",
    "        self.memory[idx,:] = row_entry\n",
    "        \n",
    "        self.memory_counter+=1\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        idx = np.random.choice(self.memory_capacity, self.batch_size)\n",
    "        memory_batch = self.memory[idx,:]\n",
    "        \n",
    "        s = memory_batch[:,:self.n_states]\n",
    "        a = memory_batch[:,self.n_states]\n",
    "        r = memory_batch[:,self.n_states+1]\n",
    "        s_ = memory_batch[:,-self.n_states:]\n",
    "        \n",
    "        loss,_ = self.sess.run([self.loss,self.step],feed_dict={self.s:s,\n",
    "                                                               self.a:a,\n",
    "                                                               self.r:r,\n",
    "                                                               self.s_:s_})\n",
    "\n",
    "        \n",
    "        \n",
    "        # 固定步數執行同步動作\n",
    "        if self.synchronize_counter%self.synchronize_iter==0:\n",
    "            print(\"eval參數 同步到 target參數\")\n",
    "            self.sess.run(self.synchronize_op)\n",
    "            \n",
    "        self.synchronize_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DQN = dueling_double_DQN_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval參數 同步到 target參數\n",
      "Ep:  3 | Ep_r:  -3.29\n",
      "Ep:  4 | Ep_r:  1.86\n",
      "Ep:  5 | Ep_r:  2.06\n",
      "Ep:  6 | Ep_r:  1.08\n",
      "Ep:  7 | Ep_r:  1.03\n",
      "Ep:  8 | Ep_r:  4.13\n",
      "Ep:  9 | Ep_r:  1.07\n",
      "Ep:  10 | Ep_r:  1.56\n",
      "Ep:  11 | Ep_r:  1.04\n",
      "Ep:  12 | Ep_r:  2.99\n",
      "eval參數 同步到 target參數\n",
      "Ep:  13 | Ep_r:  2.55\n",
      "Ep:  14 | Ep_r:  1.64\n",
      "Ep:  15 | Ep_r:  9.08\n",
      "Ep:  16 | Ep_r:  2.74\n",
      "Ep:  17 | Ep_r:  2.89\n",
      "Ep:  18 | Ep_r:  1.96\n",
      "eval參數 同步到 target參數\n",
      "Ep:  19 | Ep_r:  5.63\n",
      "Ep:  20 | Ep_r:  4.18\n",
      "Ep:  21 | Ep_r:  1.2\n",
      "Ep:  22 | Ep_r:  -1.19\n",
      "eval參數 同步到 target參數\n",
      "Ep:  23 | Ep_r:  -3.76\n",
      "Ep:  24 | Ep_r:  -2.37\n",
      "eval參數 同步到 target參數\n",
      "Ep:  25 | Ep_r:  2.36\n",
      "Ep:  26 | Ep_r:  12.79\n",
      "eval參數 同步到 target參數\n",
      "Ep:  27 | Ep_r:  8.71\n",
      "Ep:  28 | Ep_r:  3.97\n",
      "Ep:  29 | Ep_r:  4.58\n",
      "Ep:  30 | Ep_r:  2.65\n",
      "Ep:  31 | Ep_r:  3.94\n",
      "Ep:  32 | Ep_r:  2.87\n",
      "Ep:  33 | Ep_r:  2.17\n",
      "Ep:  34 | Ep_r:  2.77\n",
      "Ep:  35 | Ep_r:  1.4\n",
      "eval參數 同步到 target參數\n",
      "Ep:  36 | Ep_r:  3.35\n",
      "Ep:  37 | Ep_r:  0.59\n",
      "eval參數 同步到 target參數\n",
      "Ep:  38 | Ep_r:  23.21\n",
      "eval參數 同步到 target參數\n",
      "Ep:  39 | Ep_r:  60.32\n",
      "eval參數 同步到 target參數\n",
      "Ep:  40 | Ep_r:  61.42\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  41 | Ep_r:  9.32\n",
      "eval參數 同步到 target參數\n",
      "Ep:  42 | Ep_r:  52.37\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  43 | Ep_r:  75.98\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  44 | Ep_r:  54.11\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  45 | Ep_r:  123.42\n",
      "eval參數 同步到 target參數\n",
      "Ep:  46 | Ep_r:  25.85\n",
      "eval參數 同步到 target參數\n",
      "Ep:  47 | Ep_r:  40.25\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  48 | Ep_r:  58.34\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  49 | Ep_r:  108.13\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  50 | Ep_r:  67.31\n",
      "eval參數 同步到 target參數\n",
      "Ep:  51 | Ep_r:  38.4\n",
      "eval參數 同步到 target參數\n",
      "Ep:  52 | Ep_r:  33.63\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  53 | Ep_r:  72.23\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  54 | Ep_r:  74.87\n",
      "eval參數 同步到 target參數\n",
      "Ep:  55 | Ep_r:  31.16\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  56 | Ep_r:  69.31\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  57 | Ep_r:  94.55\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  58 | Ep_r:  108.67\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  59 | Ep_r:  91.12\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  60 | Ep_r:  58.99\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  61 | Ep_r:  143.41\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  62 | Ep_r:  96.56\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  63 | Ep_r:  135.23\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  64 | Ep_r:  213.22\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  65 | Ep_r:  66.6\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  66 | Ep_r:  99.17\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  67 | Ep_r:  89.46\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  68 | Ep_r:  134.42\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  69 | Ep_r:  131.23\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(70):\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        a = DQN.choose_action(s)\n",
    "#         print(a)\n",
    "        # take action\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        # modify the reward\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        \n",
    "    \n",
    "        # reward要重新定義 預設是 : Reward is 1 for every step taken, including the termination step\n",
    "        ## env.x_threshold代表x方向的最大距離,r1越大代表越靠近中間\n",
    "        ## -0.8是要讓reward 不要那麼通膨, 最中間也只能拿 1-0.8 = 0.2 reward\n",
    "        ## -0.8不加也是可以train的起來的\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold -0.8\n",
    "        \n",
    "        ## 角度越接近正垂直，r2越大\n",
    "        ## -0.5同上是修正值\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians -0.5\n",
    "        r = r1 + r2\n",
    "\n",
    "        DQN.store_entry(s, a, r, s_)\n",
    "\n",
    "        ep_r += r\n",
    "        if DQN.memory_counter > DQN.memory_capacity:\n",
    "            DQN.train()\n",
    "            if done:\n",
    "                print('Ep: ', i_episode,\n",
    "                      '| Ep_r: ', round(ep_r, 2))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        s = s_\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
