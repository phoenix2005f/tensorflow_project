{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phx/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPISILON = 0.9\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "MEMORY_CAPACITY = 150\n",
    "GAMMA = 0.9\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class double_DQN_Agent():\n",
    "    def __init__(self,\n",
    "                 n_actions = 2,\n",
    "                 n_states = 4,\n",
    "                 memory_capacity = 150,\n",
    "                 episilon = 0.9,\n",
    "                 gamma = 0.9,\n",
    "                 batch_size = 32,\n",
    "                 lr = 0.01,\n",
    "                 memory_counter = 0,\n",
    "                 synchronize_iter = 100,\n",
    "                 \n",
    "                 ):\n",
    "        \n",
    "        self.memory = np.zeros((memory_capacity,n_states*2+2))\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.memory_counter = 0\n",
    "        self.synchronize_iter = synchronize_iter\n",
    "        self.synchronize_counter = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.episilon = episilon\n",
    "        \n",
    "        self._build_net()\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_net(self):\n",
    "        self.s = tf.placeholder(tf.float32,[None,self.n_states],name = \"current_state\")\n",
    "        self.a = tf.placeholder(tf.int32,[None,],name = \"action\")\n",
    "        self.r = tf.placeholder(tf.float32,[None,] ,name = \"reward\")\n",
    "        self.s_ = tf.placeholder(tf.float32,[None,self.n_states],name = \"next_state\")\n",
    "        \n",
    "        init_weight,init_bias = tf.truncated_normal_initializer(stddev=0.01),tf.constant_initializer(0.1)\n",
    "        \n",
    "        # eval_net\n",
    "        with tf.variable_scope(\"eval_net\"):\n",
    "            \n",
    "            self.e_fc1 = tf.layers.dense(self.s , 50 , activation = tf.nn.relu , \n",
    "                            kernel_initializer = init_weight , \n",
    "                            bias_initializer = init_bias, name=\"e_fc1\")\n",
    "            \n",
    "            self.e_out = tf.layers.dense(self.e_fc1 , self.n_actions , activation=tf.nn.relu,\n",
    "                                   kernel_initializer = init_weight,\n",
    "                                   bias_initializer = init_bias , \n",
    "                                    name=\"e_fc2\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        # target_net\n",
    "        with tf.variable_scope(\"target_net\"):\n",
    "            self.t_fc1 = tf.layers.dense(self.s_ , 50 , activation =tf.nn.relu,\n",
    "                                        kernel_initializer = init_weight , \n",
    "                                        bias_initializer = init_bias , name=\"t_fc1\")\n",
    "            \n",
    "            self.t_out = tf.layers.dense(self.t_fc1 , self.n_actions , activation=tf.nn.relu,\n",
    "                                        kernel_initializer = init_weight,\n",
    "                                        bias_initializer = init_bias , name= \"t_fc2\")\n",
    "        \n",
    "        \n",
    "        self.eval_params = tf.get_collection(key = tf.GraphKeys.TRAINABLE_VARIABLES , scope = \"eval_net\")\n",
    "        self.target_params = tf.get_collection(key = tf.GraphKeys.TRAINABLE_VARIABLES , scope = \"target_net\")\n",
    "        \n",
    "        \n",
    "        # eval 參數同步到 target的op\n",
    "        self.synchronize_op = [ tf.assign(t,e) for e,t in zip(self.eval_params,self.target_params)]\n",
    "        \n",
    "        # loss\n",
    "        \n",
    "        \n",
    "#         next_q = tf.reduce_max(self.t_out,axis=1)\n",
    "        ######### double QDN只要改這邊 #########\n",
    "        # 用eval net得到的batch action,再利用target net得到batch action所對應的Q值\n",
    "        # 會加速不少\n",
    "        \n",
    "        e_actions = tf.argmax(self.e_out,axis=1,output_type=tf.int32)\n",
    "        e_actions_idx = tf.stack([tf.range(tf.shape(e_actions)[0]),e_actions],axis=1)\n",
    "        next_q=tf.gather_nd(params=self.t_out,indices = e_actions_idx)\n",
    "        \n",
    "        ######################################\n",
    "        \n",
    "        self.target_q = self.r + self.gamma*next_q\n",
    "#         self.target_q = tf.stop_gradient(self.target_q)  # target不要更新參數\n",
    "        \n",
    "        index = tf.stack([tf.range(tf.shape(self.a)[0]),self.a],axis=1)\n",
    "        self.eval_q = tf.gather_nd(params = self.e_out , indices = index)\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.eval_q,self.target_q))\n",
    "        \n",
    "        # 只更新eval net的參數\n",
    "        q_vars = [ var for var in tf.trainable_variables() if \"eval_net\" in var.name]\n",
    "        self.step = tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.loss,var_list=q_vars)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def choose_action(self,states):\n",
    "        states = states[np.newaxis,:]\n",
    "        \n",
    "        q_val = self.sess.run(self.e_out,feed_dict={self.s:states})\n",
    "        if np.random.normal() < self.episilon:\n",
    "            act = np.argmax(q_val,axis=1)[0]\n",
    "        else:\n",
    "            act = np.random.choice(self.n_actions)\n",
    "        return act\n",
    "        \n",
    "    def store_entry(self,s,a,r,s_):\n",
    "        idx = self.memory_counter % self.memory_capacity\n",
    "        \n",
    "        row_entry = np.hstack([s,a,r,s_])\n",
    "        self.memory[idx,:] = row_entry\n",
    "        \n",
    "        self.memory_counter+=1\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        idx = np.random.choice(self.memory_capacity, self.batch_size)\n",
    "        memory_batch = self.memory[idx,:]\n",
    "        \n",
    "        s = memory_batch[:,:self.n_states]\n",
    "        a = memory_batch[:,self.n_states]\n",
    "        r = memory_batch[:,self.n_states+1]\n",
    "        s_ = memory_batch[:,-self.n_states:]\n",
    "        \n",
    "        loss,_ = self.sess.run([self.loss,self.step],feed_dict={self.s:s,\n",
    "                                                               self.a:a,\n",
    "                                                               self.r:r,\n",
    "                                                               self.s_:s_})\n",
    "\n",
    "        \n",
    "        \n",
    "        # 固定步數執行同步動作\n",
    "        if self.synchronize_counter%self.synchronize_iter==0:\n",
    "            print(\"eval參數 同步到 target參數\")\n",
    "            self.sess.run(self.synchronize_op)\n",
    "            \n",
    "        self.synchronize_counter+=1   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DQN = double_DQN_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval參數 同步到 target參數\n",
      "Ep:  15 | Ep_r:  4.03\n",
      "Ep:  16 | Ep_r:  2.73\n",
      "Ep:  17 | Ep_r:  1.46\n",
      "Ep:  18 | Ep_r:  2.42\n",
      "Ep:  19 | Ep_r:  2.92\n",
      "Ep:  20 | Ep_r:  5.68\n",
      "Ep:  21 | Ep_r:  1.77\n",
      "Ep:  22 | Ep_r:  0.71\n",
      "Ep:  23 | Ep_r:  1.29\n",
      "eval參數 同步到 target參數\n",
      "Ep:  24 | Ep_r:  3.72\n",
      "Ep:  25 | Ep_r:  3.38\n",
      "Ep:  26 | Ep_r:  9.73\n",
      "Ep:  27 | Ep_r:  2.87\n",
      "eval參數 同步到 target參數\n",
      "Ep:  28 | Ep_r:  1.77\n",
      "Ep:  29 | Ep_r:  2.11\n",
      "Ep:  30 | Ep_r:  4.75\n",
      "Ep:  31 | Ep_r:  2.36\n",
      "Ep:  32 | Ep_r:  1.51\n",
      "Ep:  33 | Ep_r:  1.7\n",
      "Ep:  34 | Ep_r:  1.34\n",
      "Ep:  35 | Ep_r:  4.36\n",
      "eval參數 同步到 target參數\n",
      "Ep:  36 | Ep_r:  5.99\n",
      "Ep:  37 | Ep_r:  26.24\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  38 | Ep_r:  69.31\n",
      "eval參數 同步到 target參數\n",
      "Ep:  39 | Ep_r:  14.74\n",
      "Ep:  40 | Ep_r:  13.59\n",
      "Ep:  41 | Ep_r:  2.69\n",
      "Ep:  42 | Ep_r:  3.35\n",
      "eval參數 同步到 target參數\n",
      "Ep:  43 | Ep_r:  2.23\n",
      "Ep:  44 | Ep_r:  10.91\n",
      "Ep:  45 | Ep_r:  3.35\n",
      "eval參數 同步到 target參數\n",
      "Ep:  46 | Ep_r:  3.63\n",
      "eval參數 同步到 target參數\n",
      "Ep:  47 | Ep_r:  12.36\n",
      "eval參數 同步到 target參數\n",
      "Ep:  48 | Ep_r:  27.39\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  49 | Ep_r:  76.05\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  50 | Ep_r:  108.41\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  51 | Ep_r:  55.02\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  52 | Ep_r:  68.97\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  53 | Ep_r:  51.95\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  54 | Ep_r:  49.52\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  55 | Ep_r:  35.99\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  56 | Ep_r:  34.71\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  57 | Ep_r:  46.02\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  58 | Ep_r:  64.02\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  59 | Ep_r:  77.31\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  60 | Ep_r:  131.76\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  61 | Ep_r:  176.4\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  62 | Ep_r:  49.38\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  63 | Ep_r:  67.46\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  64 | Ep_r:  104.35\n",
      "Ep:  65 | Ep_r:  1.57\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  66 | Ep_r:  87.16\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  67 | Ep_r:  47.35\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  68 | Ep_r:  138.52\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  69 | Ep_r:  38.66\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  70 | Ep_r:  261.77\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  71 | Ep_r:  87.64\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  72 | Ep_r:  149.75\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  73 | Ep_r:  98.69\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  74 | Ep_r:  228.67\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  75 | Ep_r:  82.36\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  76 | Ep_r:  322.55\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  77 | Ep_r:  137.95\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  78 | Ep_r:  130.03\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  79 | Ep_r:  95.58\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  80 | Ep_r:  272.43\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  81 | Ep_r:  52.64\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  82 | Ep_r:  72.12\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  83 | Ep_r:  141.08\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  84 | Ep_r:  80.08\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  85 | Ep_r:  257.63\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  86 | Ep_r:  107.36\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  87 | Ep_r:  373.87\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  88 | Ep_r:  304.73\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n",
      "Ep:  89 | Ep_r:  94.39\n",
      "eval參數 同步到 target參數\n",
      "eval參數 同步到 target參數\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(100):\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        a = DQN.choose_action(s)\n",
    "#         print(a)\n",
    "        # take action\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        # modify the reward\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        \n",
    "    \n",
    "        # reward要重新定義 預設是 : Reward is 1 for every step taken, including the termination step\n",
    "        ## env.x_threshold代表x方向的最大距離,r1越大代表越靠近中間\n",
    "        ## -0.8是要讓reward 不要那麼通膨, 最中間也只能拿 1-0.8 = 0.2 reward\n",
    "        ## -0.8不加也是可以train的起來的\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold -0.8\n",
    "        \n",
    "        ## 角度越接近正垂直，r2越大\n",
    "        ## -0.5同上是修正值\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians -0.5\n",
    "        r = r1 + r2\n",
    "\n",
    "        DQN.store_entry(s, a, r, s_)\n",
    "\n",
    "        ep_r += r\n",
    "        if DQN.memory_counter > DQN.memory_capacity:\n",
    "            DQN.train()\n",
    "            if done:\n",
    "                print('Ep: ', i_episode,\n",
    "                      '| Ep_r: ', round(ep_r, 2))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        s = s_\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
